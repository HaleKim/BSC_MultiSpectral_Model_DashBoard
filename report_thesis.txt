## BSC 다중 스펙트럼 모델 기반 지능형 감시 시스템 상세 보고서

### 서론

본 문서는 'BSC 다중 스펙트럼 모델 대시보드' 시스템의 설계 및 구현에 대한 상세 기술 보고서이다. 본 시스템은 GOP(일반전초)와 같은 경계 감시 환경에서 실시간으로 입력되는 RGB(가시광선) 및 TIR(열화상) 영상을 융합하여 분석하고, 딥러닝 기반의 객체 탐지 모델을 통해 사람, 멧돼지, 고라니 등 주요 감시 대상의 식별 및 추적을 자동화하는 것을 목표로 한다. 이를 통해 경계 근무의 효율성을 증대시키고, 24시간 상시 감시 체계를 구축하여 잠재적 위협에 대한 대응 능력을 강화하고자 한다. 시스템은 웹 기반의 통합 대시보드를 통해 사용자에게 직관적인 인터페이스를 제공하며, 실시간 영상 스트리밍, 이벤트 발생 기록, 녹화 영상 확인 및 시스템 관리 기능을 포함한다.

---

### 1. 시스템 아키텍처 설계

본 시스템은 현대적인 웹 애플리케이션의 표준으로 자리 잡은 클라이언트-서버 아키텍처를 채택하였다. 이 구조는 역할과 책임에 따라 시스템을 논리적으로 분리함으로써 유연성, 확장성, 그리고 유지보수성을 극대화한다. 각 구성 요소의 역할과 상호작용은 다음과 같다.

#### 1.1. 프론트엔드 (클라이언트 계층)
사용자와 직접 상호작용하는 부분으로, React.js 라이브러리를 기반으로 한 SPA(Single Page Application)로 구현되었다. 사용자는 웹 브라우저를 통해 시스템에 접근하며, 프론트엔드는 백엔드 서버로부터 제공받은 데이터를 시각화하고 사용자 입력을 처리하여 서버에 전달하는 역할을 담당한다. 비동기 통신(AJAX)과 웹소켓을 통해 동적인 사용자 경험을 제공한다.

#### 1.2. 백엔드 (서버 계층)
시스템의 모든 핵심 비즈니스 로직이 수행되는 중앙 서버로, Python 언어와 Flask 웹 프레임워크를 기반으로 구축되었다. 백엔드의 주요 책임은 다음과 같다.
- **API 서버**: 프론트엔드의 요청에 응답하는 RESTful API를 제공한다. (사용자 인증, 데이터 조회/관리 등)
- **비디오 처리**: 실시간 카메라(RTSP) 또는 저장된 영상 파일로부터 비디오 스트림을 수신하여 프레임 단위로 분할하고 처리한다.
- **AI 연동**: 처리된 영상 프레임을 AI 객체 탐지 모델에 전달하여 추론을 수행하고, 그 결과를 반환받는다.
- **데이터베이스 연동**: 시스템 운영에 필요한 모든 데이터(사용자, 이벤트 등)를 데이터베이스에 저장하고 관리한다.
- **실시간 통신 서버**: WebSocket 프로토콜을 통해 프론트엔드 클라이언트와 양방향 통신 채널을 유지하며, 실시간 영상 프레임 및 이벤트 알림을 전송한다.

#### 1.3. 데이터베이스 계층
시스템의 모든 정형 데이터를 영구적으로 저장하는 공간이다. 관계형 데이터베이스 관리 시스템(RDBMS)인 MySQL을 사용하며, SQLAlchemy와 같은 ORM(Object-Relational Mapper)을 통해 백엔드 애플리케이션과 상호작용한다. 이를 통해 데이터의 무결성과 일관성을 보장한다.

#### 1.4. 데이터 흐름 및 상호작용
1.  **사용자 인증**: 사용자가 프론트엔드에서 로그인하면, 백엔드는 데이터베이스의 사용자 정보와 대조하여 인증을 수행하고, 성공 시 JWT(JSON Web Token)를 발급한다.
2.  **스트리밍 요청**: 사용자가 대시보드에서 실시간 영상 보기를 시작하면, 프론트엔드는 WebSocket을 통해 백엔드에 스트리밍 시작을 요청한다.
3.  **영상 처리 및 분석**: 백엔드는 해당 카메라의 영상 스트림을 받아 프레임 단위로 AI 모델에 전달한다. AI 모델은 객체 탐지를 수행하고 Bounding Box, 객체 종류, 신뢰도 등의 정보를 포함한 분석 결과를 반환한다.
4.  **실시간 전송**: 백엔드는 분석 결과가 포함된 영상 프레임을 JPEG 이미지로 인코딩하여 WebSocket을 통해 프론트엔드로 실시간 전송한다.
5.  **이벤트 발생 및 저장**: 만약 분석 결과가 특정 조건(예: '사람' 객체가 70% 이상의 신뢰도로 탐지)을 만족하면, 백엔드는 이를 '탐지 이벤트'로 규정한다. 해당 이벤트 정보(시각, 카메라, 객체 정보 등)를 데이터베이스에 기록하고, 이벤트 발생 전후의 영상을 별도 파일로 녹화하여 저장한다. 동시에, WebSocket을 통해 프론트엔드에 새로운 이벤트가 발생했음을 알린다.
6.  **정보 조회**: 프론트엔드는 이벤트 목록, 사용자 정보 등 필요한 데이터를 백엔드의 RESTful API를 통해 비동기적으로 요청하고 응답받아 화면에 표시한다.

---

### 2. 데이터베이스 구현

데이터베이스는 시스템의 상태와 기록을 저장하는 핵심 요소로, `Flask-SQLAlchemy` 라이브러리를 통해 객체 지향적인 방식으로 모델링 및 관리된다. `backend/app/models/db_models.py` 파일에 정의된 주요 스키마는 다음과 같다.

-   **User (users 테이블)**: 시스템 사용자의 정보를 관리한다. `username`은 시스템 내에서 유일해야 하며, `role` 필드를 통해 'ADMIN'과 'USER' 권한을 구분하여 접근 제어를 구현한다.
-   **Camera (cameras 테이블)**: 감시 카메라의 물리적, 논리적 정보를 저장한다. `source` 필드에는 RTSP 주소와 같은 영상 스트림 원본 경로가 저장되어, 백엔드가 해당 카메라의 영상을 가져오는 데 사용된다.
-   **DetectionEvent (detection_events 테이블)**: 시스템의 가장 중요한 로그 데이터로, AI 모델에 의해 생성된 모든 탐지 이벤트를 기록한다. 이 테이블은 `User`와 `Camera` 테이블을 외래 키로 참조하여, '어떤 근무자가', '어느 카메라에서', '무엇을' 탐지했는지의 관계를 명확히 나타낸다.
-   **EventFile (event_files 테이블)**: 하나의 탐지 이벤트에 대해 여러 종류의 미디어 파일(예: 원본 녹화 영상, 썸네일 이미지)이 생성될 수 있으므로, 이를 관리하기 위한 별도의 테이블이다. `DetectionEvent`와 1:N 관계를 맺는다.

이러한 관계형 모델 설계는 데이터의 정규화를 통해 중복을 최소화하고, 외래 키 제약을 통해 데이터의 무결성을 보장한다.

---

### 3. 서버 구현

백엔드 서버는 Flask 기반으로 각 기능이 Blueprint로 모듈화되어 있으며, 핵심 로직은 서비스 계층에 구현되어 있다.

#### 3.1. RESTful API (`/app/api/routes.py`)
-   **인증**: `Flask-JWT-Extended`를 사용하여 API 접근을 보호한다. 로그인 성공 시 발급된 JWT는 이후 모든 API 요청의 `Authorization` 헤더에 포함되어야 하며, 서버는 각 요청마다 토큰의 유효성을 검증한다.
-   **자원 관리**: 사용자, 카메라, 이벤트 등 시스템의 주요 자원(Resource)에 대해 생성(POST), 조회(GET), 삭제(DELETE) 등의 CRUD(Create, Read, Update, Delete) 연산을 수행하는 엔드포인트를 제공한다. 관리자 전용 기능은 `@admin_required` 데코레이터를 통해 엄격하게 접근이 통제된다.

#### 3.2. 실시간 통신 (`/app/sockets/events.py`)
-   `Flask-SocketIO`를 사용하여 구현되었으며, `eventlet`을 워커로 사용하여 높은 동시성 처리 능력을 확보했다.
-   클라이언트(`request.sid`)별로 비디오 처리 작업을 별도의 스레드(eventlet greenlet)에서 관리하여, 다수의 사용자가 동시에 각기 다른 영상 스트림을 요청하더라도 독립적으로 처리될 수 있도록 보장한다.
-   `start_stream`과 `stop_stream` 이벤트 핸들러는 이러한 비디오 처리 스레드의 생명주기를 관리한다.

#### 3.3. 핵심 서비스 로직 (`/app/services/video_service.py`)
-   **다중 스펙트럼 영상 처리**: 시스템의 핵심 기능으로, RGB와 TIR 두 종류의 영상 소스를 동시에 처리한다. `cv2.VideoCapture`를 통해 각 소스로부터 프레임을 읽어온 후, AI 모델의 입력 요구사항에 맞게 NumPy 배열을 사용하여 두 프레임을 채널(channel) 기준으로 결합(concatenate)한다. 이는 단일 스펙트럼 영상만 사용하는 것보다 다양한 환경(특히 야간)에서의 탐지 성능을 향상시킨다.
-   **AI 추론**: `ultralytics` 라이브러리를 통해 사전 훈련된 YOLO 모델(`*.pt`)을 로드한다. 준비된 융합 프레임을 `model.track()` 메소드에 전달하여 객체 탐지 및 추적을 수행한다. `persist=True` 옵션을 통해 프레임 간 객체 추적을 유지한다.
-   **이벤트 기반 녹화**: 효율적인 메모리 관리를 위해 `collections.deque`를 순환 버퍼(circular buffer)로 사용한다. 이 버퍼는 항상 최신 10-12초 분량의 영상 프레임을 저장하고 있다. 이벤트가 발생하는 즉시, 버퍼에 저장된 '이전 10초'의 영상과, 이벤트 발생 후 '이후 10초' 동안 수집되는 영상을 합쳐 총 20초의 클립을 생성한다. 영상 저장은 별도의 스레드에서 비동기적으로 처리되어 메인 영상 처리 루프의 지연을 방지한다.

---

### 4. 사용자 인터페이스 및 통합 기능

프론트엔드는 사용자에게 시스템의 모든 기능을 직관적으로 제공하는 접점이다.

#### 4.1. 컴포넌트 기반 설계
-   UI를 재사용 가능한 독립적인 단위인 컴포넌트로 분할하여 개발 효율성과 유지보수성을 높였다. 예를 들어, `VideoStream` 컴포넌트는 영상 종류(RGB/TIR)에 관계없이 영상 데이터를 받아 화면에 표시하는 역할만 담당하며, `EventList`는 이벤트 데이터 배열을 받아 목록으로 렌더링하는 데 집중한다.

#### 4.2. 상태 관리 및 라우팅
-   **상태 관리**: `React Context API`를 사용하여 사용자 인증 정보와 같은 전역 상태를 관리한다. `AuthContext`는 애플리케이션 전반에 걸쳐 로그인한 사용자 정보와 JWT 토큰을 제공하며, 하위 컴포넌트들은 이 정보를 쉽게 소비할 수 있다.
-   **라우팅**: `react-router-dom` 라이브러리를 사용하여 클라이언트 사이드 라우팅을 구현했다. 이를 통해 페이지 새로고침 없이 URL에 따라 다른 페이지 컴포넌트(`LoginPage`, `MainPage`, `AdminPage`)를 렌더링하여 SPA의 장점을 극대화한다. `ProtectedRoute`와 `AdminRoute`는 `AuthContext`의 상태를 확인하여 특정 경로에 대한 접근 권한을 제어하는 역할을 수행한다.

#### 4.3. 백엔드 연동
-   **REST API 연동**: `axios` 라이브러리를 사용하여 백엔드 API와 통신한다. `axios` 인스턴스를 생성하고 인터셉터(interceptor)를 설정하여, 모든 요청에 자동으로 JWT 토큰을 `Authorization` 헤더에 추가하는 로직을 중앙에서 관리한다.
-   **WebSocket 연동**: `socket.io-client`를 사용하여 백엔드의 Socket.IO 서버와 연결을 수립한다. `socket.on()` 메소드를 통해 `video_frame`, `new_event`와 같은 이벤트를 수신 대기하고, 수신된 데이터를 React 컴포넌트의 상태(state)에 반영하여 UI를 실시간으로 업데이트한다.

### 결론

본 시스템은 다중 스펙트럼 영상 융합과 딥러닝 기술을 웹 기반 대시보드와 성공적으로 통합하여, 지능형 경계 감시를 위한 효과적인 솔루션을 구현하였다. 모듈화된 아키텍처와 표준 기술 스택을 채택함으로써 향후 기능 확장 및 성능 고도화를 위한 유연한 기반을 마련하였다. 향후 모델 최적화를 통한 탐지 정확도 향상, 클라우드 기반 배포를 통한 확장성 확보, 그리고 다양한 종류의 알림(SMS, 모바일 푸시 등) 기능 추가 등의 방향으로 발전시킬 수 있을 것이다.
